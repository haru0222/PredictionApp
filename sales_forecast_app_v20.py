# sales_forecast_app_v20.py
# äºˆæ¸¬çµæœã‚’ç”»é¢ä¸‹ã«ã€Œã‚¨ã‚¯ã‚»ãƒ«ã«ã‚³ãƒ”ãƒšã§ãã‚‹è¡¨ã€ã§å‡ºåŠ›ï¼ˆExcelæ›¸ãè¾¼ã¿ã¯ã—ãªã„ï¼‰

import streamlit as st
import pandas as pd
import datetime
import requests
import joblib
import os
import numpy as np
import jpholiday

# --- æ—¥æœ¬èªã‚µã‚¤ãƒˆã®ã‚¤ãƒ™ãƒ³ãƒˆ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆ/ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£/ãŠå°å ´ï¼‰ ---
import re
from functools import lru_cache
from bs4 import BeautifulSoup

# åé›†å¯¾è±¡URLï¼ˆæ—¥æœ¬èªã®ã¿ï¼‰
EVENT_SOURCES_JP = [
    # æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆï¼ˆã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ï¼‰
    ("https://www.bigsight.jp/visitor/event/", "æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆ"),
    # ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£æ±äº¬ãƒ—ãƒ©ã‚¶ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ï¼‰
    ("https://mitsui-shopping-park.com/divercity-tokyo/event/", "ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£æ±äº¬ãƒ—ãƒ©ã‚¶"),
    # ãŠå°å ´ å…¬å¼ãƒãƒ¼ã‚¿ãƒ«ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§ãƒ»ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ï¼‰
    ("https://www.tokyo-odaiba.net/event_index/", "ãŠå°å ´ï¼ˆå…¬å¼ä¸€è¦§ï¼‰"),
    ("https://www.tokyo-odaiba.net/event_calender/", "ãŠå°å ´ï¼ˆå…¬å¼ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ï¼‰"),
    # Zepp DiverCityï¼ˆãƒ©ã‚¤ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰
    ("https://www.zepp.co.jp/hall/divercity/schedule/", "Zepp DiverCity"),
]

# --------------------------------------------
# å¤§è¦æ¨¡ã‚¤ãƒ™ãƒ³ãƒˆã ã‘ã«çµã‚‹ãƒ•ã‚£ãƒ«ã‚¿é–¢æ•°
# --------------------------------------------
BIG_EVENT_KEYWORDS = [
    "ãƒ•ã‚§ã‚¹", "ãƒ•ã‚§ã‚¹ã‚¿", "ç¥­",
    "åš", "åšè¦§ä¼š", "å±•ç¤ºä¼š", "è¦‹æœ¬å¸‚",
    "ã‚¨ã‚­ã‚¹ãƒ", "EXPO",
    "ã‚³ãƒŸãƒƒã‚¯ãƒãƒ¼ã‚±ãƒƒãƒˆ", "ã‚³ãƒŸã‚±",
    "ãƒ•ã‚§ã‚¢", "ã‚·ãƒ§ãƒ¼",
    "èŠ±ç«","èŠ±ç«å¤§ä¼š","HANABI",
]

def _filter_big_events(events):
    """
    æœ¬å½“ã«äººãŒå¤šãæ¥ã‚‹å¤§è¦æ¨¡ã‚¤ãƒ™ãƒ³ãƒˆã ã‘ã«çµã‚‹ãƒ•ã‚£ãƒ«ã‚¿
    ãƒ»UIãƒ†ã‚­ã‚¹ãƒˆ/å•†è«‡ç³»ã‚’é™¤å¤–
    ãƒ»ãŠå°å ´ãŸã“ç„¼ããƒŸãƒ¥ãƒ¼ã‚¸ã‚¢ãƒ ç³»ãªã©å¸¸è¨­å¯„ã‚Šä¼ç”»ã‚’é™¤å¤–
    ãƒ»ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ•ã‚§ã‚¹ã‚¿ï¼ã‚¢ãƒŸãƒ¥ãƒ¼ã‚ºãƒ¡ãƒ³ãƒˆã‚¨ã‚­ã‚¹ãƒç­‰ã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦é‡è¤‡å‰Šé™¤
    ãƒ»åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆä¼šå ´Ã—ã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã¯ã€ŒæœŸé–“ãŒä¸€ç•ªçŸ­ã„ã‚‚ã®ã€ã ã‘æ®‹ã™
      â†’ ãƒ‡ã‚¶ãƒ•ã‚§ã‚¹ã¿ãŸã„ã« 10/04ã€œ11/15 ã¨ã‹ 11/14ã€œ11/30 ãŒæ··åœ¨ã—ã¦ã‚‚ã€
         æœ¬ç•ªã® 11/15ã€œ11/16 ã ã‘ã‚’æ¡ç”¨ã™ã‚‹ç‹™ã„
    """
    # ã‚µã‚¤ãƒˆã®UIãƒ†ã‚­ã‚¹ãƒˆãƒ»æ¤œç´¢èª¬æ˜ãƒ»å•†è«‡ç³»ãªã©ã€æ˜ã‚‰ã‹ã«ã‚¤ãƒ™ãƒ³ãƒˆåã˜ã‚ƒãªã„ã‚‚ã®
    noise_keywords = [
        "ã‚¢ã‚¯ã‚»ã‚¹", "ãƒ•ãƒ­ã‚¢ãƒãƒƒãƒ—", "ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±", "ã‚·ãƒ§ãƒƒãƒ—ï¼†ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³",
        "ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", "æ—¥ä»˜æ¤œç´¢", "æ¤œç´¢çµæœ",
        "ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼", "ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰æ¢ã™",
        "ã‚¸ãƒ£ãƒ³ãƒ«", "æ¡ä»¶é¸æŠ",
        "ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æ¢ã™", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æ¢ã™",
        "å¹´é–“ã®ä¸»è¦ã‚¤ãƒ™ãƒ³ãƒˆ",
        "ã‚¤ãƒ™ãƒ³ãƒˆãƒ»ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³",
        "å…¥å ´åŒºåˆ†",     # å•†è«‡ç³»ã®ãƒ˜ãƒƒãƒ€ã”ã¨å…¨éƒ¨ã‚«ãƒƒãƒˆ
        "é–‹å‚¬æœŸé–“",     # ã€Œé–‹å‚¬æœŸé–“ 2025å¹´â€¦ã€ã ã‘ã®è¡Œã‚’ã‚«ãƒƒãƒˆ
        "é–‹å‚¬æ™‚é–“",     # æ™‚é–“ã ã‘ã®è¡Œ
        "å•†è«‡æ—¥æ™‚",     # å•†è«‡æ—¥æ™‚ã ã‘ã®è¡Œ
        # â€»ã€Œã‚¨ ãƒª ã‚¢ ãƒ ãƒƒ ãƒ—ã€ã¯ã‚¿ã‚¤ãƒˆãƒ«ã«ã‚‚å«ã¾ã‚Œã‚‹ã®ã§ã“ã“ã«ã¯å…¥ã‚Œãªã„
    ]

    # æ—¥å¸¸å¯„ã‚Šã®ãƒ­ãƒ³ã‚°ãƒ©ãƒ³ä¼ç”»ï¼ˆå£²ä¸Šã«ã‚ã¾ã‚ŠåŠ¹ã‹ãªãã†ãªã‚‚ã®ï¼‰
    # ãŸã ã—ã‚¿ã‚¤ãƒˆãƒ«ã«ã€Œæ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆã€ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯æ®‹ã—ãŸã„ã®ã§æ¡ä»¶ã§çµã‚‹
    small_odaiba_keywords = [
        "ãŠå°å ´ãŸã“ç„¼ããƒŸãƒ¥ãƒ¼ã‚¸ã‚¢ãƒ ",
        "å°å ´ä¸€ä¸ç›®å•†åº—è¡—",
        "ãƒ‡ãƒƒã‚¯ã‚¹æ±äº¬ãƒ“ãƒ¼ãƒ",
    ]

    # key = (ä¼šå ´, æ­£è¦åŒ–ã‚¿ã‚¤ãƒˆãƒ«) â†’ (æœŸé–“æ—¥æ•°, ã‚¤ãƒ™ãƒ³ãƒˆdict)
    # ã§ç®¡ç†ã—ã¦ã€ã€ŒåŒã˜ã‚¤ãƒ™ãƒ³ãƒˆã€ã¯ä¸€ç•ªçŸ­ã„æœŸé–“ã ã‘æ®‹ã™
    best_events = {}

    for ev in events:
        title = ev.get("ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæŠœç²‹ï¼‰", "")
        venue = ev.get("ä¼šå ´", "")
        start_s = ev.get("é–‹å§‹æ—¥") or ""
        end_s = ev.get("çµ‚äº†æ—¥") or ""

        # 0) ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚¹ã‚«ã‚¹ã‚«ãªã‚‰æ¨ã¦ã‚‹
        if not title or len(title) < 6:
            continue

        # 1) UIãƒ†ã‚­ã‚¹ãƒˆãƒ»èª¬æ˜æ–‡ã£ã½ã„ã‚‚ã®ã¯å³é™¤å¤–
        if any(k in title for k in noise_keywords):
            continue

        # 1.5) ã€Œ2025-11-14 ï½ 2025-11-15 æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆã€ã¿ãŸã„ãª
        #      æ—¥ä»˜ï¼‹ä¼šå ´ã ã‘ã§ã€ã‚¤ãƒ™ãƒ³ãƒˆåãŒç„¡ã•ãã†ãªè¡Œã‚’é™¤å¤–
        if re.match(r"\d{4}-\d{2}-\d{2}\s*ï½\s*\d{4}-\d{2}-\d{2}", title):
            continue

        # 2) æœŸé–“ï¼ˆæ—¥æ•°ï¼‰ã‚’è¨ˆç®—ï¼ˆå¤±æ•—ã—ãŸã‚‰ 9999 æ—¥æ‰±ã„ï¼‰
        try:
            start = datetime.date.fromisoformat(start_s)
            end = datetime.date.fromisoformat(end_s)
            days = (end - start).days + 1
        except Exception:
            start = end = None
            days = 9999

        # 3) ãŠå°å ´ã®å¸¸è¨­å¯„ã‚Šä¼ç”»ã‚’ã‚«ãƒƒãƒˆ
        #    ãŸã ã—ã‚¿ã‚¤ãƒˆãƒ«ã«ã€Œæ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆã€ãŒã‚ã‚‹å ´åˆã¯æ®‹ã™
        if any(k in title for k in small_odaiba_keywords) and "æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆ" not in title:
            continue

        # 4) ã©ã®ç¨‹åº¦ã€Œå¤§ãã„ã‚¤ãƒ™ãƒ³ãƒˆã€ã¨ã¿ãªã™ã‹
        keep = False

        # 4-1) æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆæœ¬ä½“ or ã‚¿ã‚¤ãƒˆãƒ«ã«ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆã¨æ›¸ã„ã¦ã‚ã‚‹ â†’ å¤§è¦æ¨¡æ‰±ã„ã§æ®‹ã™
        if venue == "æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆ" or "æ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆ" in title:
            keep = True
        else:
            # 4-2) ãã‚Œä»¥å¤–ã®ä¼šå ´ï¼ˆãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£ãƒ»é˜²ç½å…¬åœ’ãªã©ï¼‰
            #      10æ—¥ä»¥ä¸Šç¶šãé•·æœŸä¼ç”»ã¯ã€å¸¸è¨­å¯„ã‚Šã¨ã—ã¦é™¤å¤–
            if days >= 10:
                keep = False
            else:
                # ã€Œãƒ•ã‚§ã‚¹ã€ã€Œã‚¨ã‚­ã‚¹ãƒã€ãªã© â€œã‚¤ãƒ™ãƒ³ãƒˆã£ã½ã„â€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ã‚‚ã®ã ã‘æ®‹ã™
                if any(k in title for k in BIG_EVENT_KEYWORDS):
                    keep = True

        if not keep:
            continue

        # 5) ã‚¿ã‚¤ãƒˆãƒ«æ­£è¦åŒ–ï¼ˆé‡è¤‡å‰Šé™¤ç”¨ï¼‹è¡¨ç¤ºç”¨ã®æ•´ç†ï¼‰
        norm_title = title

        # ãƒ‡ã‚¶ãƒ•ã‚§ã‚¹
        if "ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ•ã‚§ã‚¹ã‚¿" in norm_title:
            norm_title = "ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ•ã‚§ã‚¹ã‚¿ vol.62 ï¼œæ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆï¼"
        # ã‚¢ãƒŸãƒ¥ãƒ¼ã‚ºãƒ¡ãƒ³ãƒˆã‚¨ã‚­ã‚¹ãƒ
        elif "ã‚¢ãƒŸãƒ¥ãƒ¼ã‚ºãƒ¡ãƒ³ãƒˆ ã‚¨ã‚­ã‚¹ãƒ" in norm_title:
            norm_title = "ã‚¢ãƒŸãƒ¥ãƒ¼ã‚ºãƒ¡ãƒ³ãƒˆ ã‚¨ã‚­ã‚¹ãƒ 2025 ï¼œæ±äº¬ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆï¼"
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°
        elif "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¢ãƒ¯ãƒ¼ãƒ‰" in norm_title:
            norm_title = "æ±äº¬å›½éš›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°ã‚¢ãƒ¯ãƒ¼ãƒ‰ Vol.10"
        # é˜²ç½ãƒ•ã‚§ã‚¹ã‚¿
        elif "é˜²ç½ãƒ•ã‚§ã‚¹ã‚¿" in norm_title:
            norm_title = "é˜²ç½ãƒ•ã‚§ã‚¹ã‚¿2025ã€Œå‚™è“„ã‚’è€ƒãˆã‚‹ã€"

        key = (venue, norm_title)

        # ã™ã§ã«åŒã˜ã‚¤ãƒ™ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€
        # ã€ŒæœŸé–“ãŒçŸ­ã„æ–¹ã€ã‚’å„ªå…ˆã—ã¦æ®‹ã™ï¼ˆæœ¬ç•ªæœŸé–“ã‚’æ¡ç”¨ã—ãŸã„ï¼‰
        prev = best_events.get(key)
        if prev is None or days < prev[0]:
            ev_new = dict(ev)  # å…ƒã®dictã‚’å£Šã•ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼
            ev_new["ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæŠœç²‹ï¼‰"] = norm_title
            best_events[key] = (days, ev_new)

    # dict ã‹ã‚‰ã‚¤ãƒ™ãƒ³ãƒˆã ã‘å–ã‚Šå‡ºã—ã¦è¿”ã™
    return [v[1] for v in best_events.values()]

# æ—¥ä»˜è¡¨è¨˜ã®ã‚†ã‚Œã«å¯¾å¿œã—ãŸæ­£è¦è¡¨ç¾ï¼ˆæ—¥æœ¬èªå¯„ã‚Šï¼‰
# ä¾‹ï¼š2025/10/1ï½2025/10/3, 2025å¹´10æœˆ1æ—¥ã€œ3æ—¥, 10/01(æ°´)ã€œ10/03(é‡‘) ãªã©
RANGE_PATTERNS = [
    r"(?P<y1>\d{4})[./å¹´\-](?P<m1>\d{1,2})[./æœˆ\-](?P<d1>\d{1,2})[æ—¥]?\s*[ï½\-â€“~ã€œè‡³ã‹ã‚‰toï½ï½â”€â€•]+\s*(?P<y2>\d{4})[./å¹´\-](?P<m2>\d{1,2})[./æœˆ\-](?P<d2>\d{1,2})[æ—¥]?",
    r"(?P<m1>\d{1,2})[./æœˆ\-](?P<d1>\d{1,2})[æ—¥]?\s*[ï½\-â€“~ã€œ]+\s*(?P<m2>\d{1,2})[./æœˆ\-](?P<d2>\d{1,2})[æ—¥]?(\s*\((?P<w2>.)\))?",
]
SINGLE_PATTERNS = [
    r"(?P<y>\d{4})[./å¹´\-](?P<m>\d{1,2})[./æœˆ\-](?P<d>\d{1,2})[æ—¥]?",
    r"(?P<m>\d{1,2})[./æœˆ\-](?P<d>\d{1,2})[æ—¥]?(?:\((?P<w>.)\))?",
]

def _to_date(y, m, d):
    return datetime.date(int(y), int(m), int(d))

def _normalize_date_str(s: str) -> str:
    # å…¨è§’ã‚„å’Œæ–‡åŒºåˆ‡ã‚Šã‚’ã–ã£ãã‚ŠASCIIå¯„ã›
    return (
        s.replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", "")
         .replace("ï¼", ".").replace("ãƒ¼", "-").replace("â€•", "-")
         .replace("ï½", "~").replace("ã€œ", "~").replace("ï¼š", ":")
    )

def _extract_date_ranges_jp(text: str, base_year: int):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ (start, end) ã®æ—¥ä»˜ãƒ¬ãƒ³ã‚¸é…åˆ—ã‚’æŠ½å‡ºï¼ˆå˜æ—¥ã¯ start=endï¼‰"""
    t = _normalize_date_str(text)

    ranges = []

    # ç¯„å›²è¡¨è¨˜
    for pat in RANGE_PATTERNS:
        for m in re.finditer(pat, t):
            gd = m.groupdict()
            try:
                if "y1" in gd and gd.get("y1") and gd.get("y2"):
                    y1, m1, d1 = gd["y1"], gd["m1"], gd["d1"]
                    y2, m2, d2 = gd["y2"], gd["m2"], gd["d2"]
                else:
                    # å¹´çœç•¥ â†’ åŒä¸€å¹´ã¨ã—ã¦æ‰±ã†ï¼ˆå¹´è·¨ãã¯è©³ç´°ãƒšãƒ¼ã‚¸ã§æ‹¾ã†ã®ãŒç¢ºå®Ÿï¼‰
                    y1 = y2 = str(base_year)
                    m1, d1 = gd["m1"], gd["d1"]
                    m2, d2 = gd["m2"], gd["d2"]

                a = _to_date(y1, m1, d1)
                b = _to_date(y2, m2, d2)
                if a <= b:
                    ranges.append((a, b))
            except Exception:
                pass

    # å˜æ—¥è¡¨è¨˜
    singles = []
    for pat in SINGLE_PATTERNS:
        for m in re.finditer(pat, t):
            gd = m.groupdict()
            try:
                if gd.get("y"):
                    d = _to_date(gd["y"], gd["m"], gd["d"])
                else:
                    d = _to_date(base_year, gd["m"], gd["d"])
                singles.append(d)
            except Exception:
                pass

    # æ—¢å­˜ãƒ¬ãƒ³ã‚¸ã«å«ã¾ã‚Œã¦ã„ãªã‘ã‚Œã°å˜æ—¥â†’ãƒ¬ãƒ³ã‚¸åŒ–
    for d in singles:
        if not any(a <= d <= b for a, b in ranges):
            ranges.append((d, d))

    return ranges

@lru_cache(maxsize=64)
def _fetch_html(url: str) -> str:
    try:
        r = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        if r.ok:
            return r.text
    except Exception:
        pass
    return ""

def _scan_event_pages_jp(target_date: datetime.date):
    """ä¸Šè¨˜æ—¥æœ¬èªãƒšãƒ¼ã‚¸ã‚’èµ°æŸ»ã—ã€target_date ã‚’å«ã‚€ã‚¤ãƒ™ãƒ³ãƒˆå€™è£œã‚’è¿”ã™"""
    hits = []
    for url, site in EVENT_SOURCES_JP:
        html = _fetch_html(url)
        if not html:
            continue
        soup = BeautifulSoup(html, "html.parser")

        # å¤§ã¾ã‹ã«è¦‹å‡ºã—ï¼‹æœ¬æ–‡ã®å¡Šã‚’èµ°æŸ»ï¼ˆã‚µã‚¤ãƒˆæ§‹é€ ã«ä¾ã‚‰ãšæ‹¾ãˆã‚‹æ±ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        for node in soup.find_all(["h1", "h2", "h3", "h4", "p", "li", "div", "a", "span"]):
            text = " ".join(node.get_text(" ", strip=True).split())
            if not text or len(text) < 6:
                continue
            ranges = _extract_date_ranges_jp(text, base_year=target_date.year)
            for a, b in ranges:
                if a <= target_date <= b:
                    title = text
                    # ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆæŠœç²‹ï¼‰ãŒé•·ã™ãã‚‹å ´åˆã¯é©åº¦ã«ä¸¸ã‚ã‚‹
                    if len(title) > 120:
                        title = title[:117] + "..."
                    hits.append({
                        "ä¼šå ´": site,
                        "é–‹å§‹æ—¥": a.isoformat(),
                        "çµ‚äº†æ—¥": b.isoformat(),
                        "ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæŠœç²‹ï¼‰": title,
                        "ãƒªãƒ³ã‚¯": url,
                    })
                    break  # ãã®ãƒãƒ¼ãƒ‰ã‹ã‚‰ã¯1ä»¶ã ã‘æ‹¾ã†

    # é‡è¤‡é™¤å»ï¼ˆä¼šå ´Ã—æœŸé–“Ã—æŠœç²‹ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–ï¼‰
    uniq, seen = [], set()
    for h in hits:
        key = (h["ä¼šå ´"], h["é–‹å§‹æ—¥"], h["çµ‚äº†æ—¥"], h["ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæŠœç²‹ï¼‰"])
        if key not in seen:
            seen.add(key)
            uniq.append(h)
    return uniq

# è¿½åŠ ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆä¸–ç•Œã®ç¥æ—¥ï¼‰
try:
    import holidays as pyholidays
except Exception:
    pyholidays = None

# å›½ã‚³ãƒ¼ãƒ‰â†’è¡¨ç¤ºåï¼ˆå¿…è¦ãªå›½ã¯ã“ã“ã«è¶³ã›ã¾ã™ï¼‰
COUNTRIES = {
    "JP": "æ—¥æœ¬", "CN": "ä¸­å›½", "US": "ã‚¢ãƒ¡ãƒªã‚«", "KR": "éŸ“å›½",
    "TW": "å°æ¹¾", "HK": "é¦™æ¸¯", "SG": "ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«", "TH": "ã‚¿ã‚¤", "VN": "ãƒ™ãƒˆãƒŠãƒ ",
    "MY": "ãƒãƒ¬ãƒ¼ã‚·ã‚¢", "ID": "ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢", "PH": "ãƒ•ã‚£ãƒªãƒ”ãƒ³",
    "GB": "ã‚¤ã‚®ãƒªã‚¹", "FR": "ãƒ•ãƒ©ãƒ³ã‚¹", "DE": "ãƒ‰ã‚¤ãƒ„", "IT": "ã‚¤ã‚¿ãƒªã‚¢", "ES": "ã‚¹ãƒšã‚¤ãƒ³",
    "CA": "ã‚«ãƒŠãƒ€", "AU": "ã‚ªãƒ¼ã‚¹ãƒˆãƒ©ãƒªã‚¢", "IN": "ã‚¤ãƒ³ãƒ‰", "BR": "ãƒ–ãƒ©ã‚¸ãƒ«"
}

# åœ°åŸŸå·®ãŒå¤§ãã„å›½ã®å·ãƒ»çœã‚³ãƒ¼ãƒ‰ã‚’å¿…è¦ãªã‚‰æŒ‡å®šï¼ˆä¾‹ï¼šUS:CA ãªã©ï¼‰
SUBDIV = {
    # "US": "CA",
    # "CN": None,
}

# ========= æ—¥æœ¬ã®é•·æœŸä¼‘ã¿ãƒ»ç¹å¿™æœŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def get_all_long_holidays(year):
    obon_days = [datetime.date(year, 8, d) for d in range(13, 17)]
    all_days = [datetime.date(year, 1, 1) + datetime.timedelta(days=i) for i in range(370)]

    def is_holiday_like(d):
        return jpholiday.is_holiday(d) or d.weekday() >= 5 or d in obon_days

    long_holidays = set()
    current_block = []
    for d in all_days:
        if is_holiday_like(d):
            current_block.append(d)
        else:
            if len(current_block) >= 3:
                long_holidays.update(current_block)
            current_block = []
    if len(current_block) >= 3:
        long_holidays.update(current_block)

    # å­¦æ ¡ã®é•·æœŸä¼‘ã¿
    summer = [datetime.date(year, 7, d) for d in range(20, 32)] + [datetime.date(year, 8, d) for d in range(1, 32)]
    winter = [datetime.date(year, 12, d) for d in range(25, 32)] + [datetime.date(year + 1, 1, d) for d in range(1, 8)]
    spring = [datetime.date(year, 3, d) for d in range(20, 32)] + [datetime.date(year, 4, d) for d in range(1, 6)]
    long_holidays.update(summer + winter + spring)
    return long_holidays

@lru_cache(maxsize=8)
def _long_holiday_days_for_year(year: int):
    return get_all_long_holidays(year)

def is_long_holiday(date):
    # å¹´è¶Šã—å¯¾å¿œï¼ˆé¸æŠæ—¥ä»˜ã®å¹´ã§åˆ¤å®šï¼‰
    return date in _long_holiday_days_for_year(date.year)

def is_crowded_day(date):
    y = date.year
    return (
        datetime.date(y, 7, 20) <= date <= datetime.date(y, 8, 31) or
        datetime.date(y, 12, 25) <= date <= datetime.date(y + 1, 1, 7) or
        datetime.date(y, 3, 20) <= date <= datetime.date(y, 4, 5) or
        datetime.date(y, 8, 13) <= date <= datetime.date(y, 8, 16) or
        datetime.date(y, 4, 29) <= date <= datetime.date(y, 5, 6)
    )

# ========= ä¸–ç•Œã®ç¥æ—¥ãƒ»é•·æœŸé€£ä¼‘ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
@lru_cache(maxsize=256)
def _country_holidays_cached(code: str, year: int, subdiv):
    """holidays.CountryHoliday ã®æ§‹ç¯‰ã‚’å¹´Ã—å›½Ã—subdivã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    if pyholidays is None:
        return None
    try:
        h = pyholidays.CountryHoliday(code, years=[year, year + 1], subdiv=subdiv)
        return h
    except Exception:
        return None

def get_international_holidays(date):
    """date ã«è©²å½“ã™ã‚‹å„å›½ã®ç¥æ—¥åã‚’ ['ä¸­å›½ï¼šæ˜¥èŠ‚', 'ã‚¢ãƒ¡ãƒªã‚«ï¼šIndependence Day', ...] å½¢å¼ã§è¿”ã™"""
    results = []
    if pyholidays is None:
        return results
    for code, label in COUNTRIES.items():
        h = _country_holidays_cached(code, date.year, SUBDIV.get(code))
        if not h:
            continue
        if date in h:
            names = h.get(date)
            if isinstance(names, (list, tuple, set)):
                name_str = "ãƒ»".join(map(str, names))
            else:
                name_str = str(names)
            results.append(f"{label}ï¼š{name_str}")
    return results

def is_long_holiday_in_country(date, country_code):
    """é€±æœ« or ãã®å›½ã®å…¬ä¼‘æ—¥ ã‚’ã€ä¼‘æ—¥ã‚‰ã—ã„æ—¥ã€ã¨ã¿ãªã—ã€3æ—¥ä»¥ä¸Šé€£ç¶šã« date ãŒå«ã¾ã‚Œã‚‹ãªã‚‰ True"""
    if country_code == "JP":
        # æ—¥æœ¬ã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãŠç›†ãƒ»å­¦æ ¡ä¼‘ã¿å«ã‚€ï¼‰ã‚’å„ªå…ˆ
        return is_long_holiday(date)
    if pyholidays is None:
        return False

    y = date.year
    hol = _country_holidays_cached(country_code, y, SUBDIV.get(country_code))
    holiday_set = set(hol.keys()) if hol else set()

    # å¹´ã‚’ã¾ãŸãå¯èƒ½æ€§ã‚ã‚Šï¼šå½“å¹´+å‰å¾Œã‚’å«ã‚ã¦èµ°æŸ»
    start = datetime.date(y, 1, 1) - datetime.timedelta(days=7)
    days = [start + datetime.timedelta(days=i) for i in range(370 + 14)]

    def is_holiday_like(d):
        return (d in holiday_set) or (d.weekday() >= 5)

    block = []
    for d in days:
        if is_holiday_like(d):
            block.append(d)
        else:
            if len(block) >= 3 and date in block:
                return True
            block = []
    # æœ«å°¾ãƒ–ãƒ­ãƒƒã‚¯
    if len(block) >= 3 and date in block:
        return True
    return False

# ========= ãƒ¢ãƒ‡ãƒ«ãƒ»å„ç¨®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ =========
sales_model = joblib.load("sales_model.pkl")
product_model_paths = joblib.load("product_model_paths.pkl")
product_models = {name: joblib.load(path) for name, path in product_model_paths.items() if os.path.exists(path)}

df_menu = pd.read_csv("å•†å“åˆ¥å£²ä¸Š_çµ±åˆ_çµ±åˆæ¸ˆv1.13.csv")
constant_items = df_menu[df_menu["æ’å¸¸ãƒ¡ãƒ‹ãƒ¥ãƒ¼"] == 1]["å•†å“å"].unique().tolist()
seasonal_items_all = df_menu[df_menu["ã‚·ãƒ¼ã‚ºãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼"] == 1]["å•†å“å"].unique().tolist()

API_KEY = st.secrets.get("OPENWEATHER_API_KEY", "")
CITY_NAME = st.secrets.get("CITY_NAME", "Odaiba,JP")
df_prev_weather = pd.read_csv("å‰å¹´_æ±äº¬ç¾½ç”°_å¤©æ°—æ°—æ¸©.csv")
df_prev_weather["date"] = pd.to_datetime(df_prev_weather["date"])

def fetch_weather_forecast(date):
    url = f"https://api.openweathermap.org/data/2.5/forecast?q={CITY_NAME}&appid={API_KEY}&units=metric&lang=ja"
    try:
        data = requests.get(url, timeout=10).json()
        for item in data.get("list", []):
            dt_ = datetime.datetime.fromtimestamp(item["dt"])
            if dt_.date() == date:
                return item["weather"][0]["main"], item["main"]["temp_max"], item["main"]["temp_min"]
    except Exception:
        pass
    return None, None, None

def make_features(entry):
    date = entry["date"]
    weekday = date.weekday()
    holiday = int(jpholiday.is_holiday(date))
    month = date.month
    season = 0 if month in [3, 4, 5] else 1 if month in [6, 7, 8] else 2 if month in [9, 10, 11] else 3
    tokui_flag = int((date.month == 2 and date.day == 14) or (date.month == 12 and date.day == 25))
    long_holiday_flag = int(is_long_holiday(date))
    busy_flag = int(is_crowded_day(date))
    weather_map = {"Clear": 0, "Clouds": 1, "Rain": 2, "æ™´ã‚Œ": 0, "æ›‡ã‚Š": 1, "é›¨": 2}

    df_feat = pd.DataFrame([{
        "æ›œæ—¥": weekday,
        "ç¥æ—¥": holiday,
        "æœ€é«˜æ°—æ¸©": entry["temp_max"],
        "æœ€ä½æ°—æ¸©": entry["temp_min"],
        "å¤©æ°—": weather_map.get(entry["weather"], 0),
        "ä¼‘æ—¥ãƒ•ãƒ©ã‚°": int(weekday >= 5 or holiday == 1),
        "ç‰¹ç•°æ—¥ãƒ•ãƒ©ã‚°": tokui_flag,
        "æœˆ": month,
        "å­£ç¯€": season,
        "ã‚¤ãƒ™ãƒ³ãƒˆæœ‰ç„¡": entry["event"],
        "é•·æœŸä¼‘ã¿ã®ç¨®é¡": 0,
        "é•·æœŸä¼‘ã¿ãƒ•ãƒ©ã‚°": long_holiday_flag,
        "ç¹å¿™æœŸãƒ•ãƒ©ã‚°": busy_flag,
        "å‰é€±åŒæ›œæ—¥_å£²ä¸Š": 200000,
        "å£²ä¸Š_ç§»å‹•å¹³å‡7æ—¥": 200000,
        "å£²ä¸Š": 200000
    }]).apply(pd.to_numeric, errors="coerce").fillna(0)
    return df_feat

# ========= å‡ºåŠ›åˆ—ï¼ˆã”æŒ‡å®šã®é †ï¼‰ =========
BASE_COLUMNS = ["æ—¥ä»˜", "æ›œæ—¥", "å¤©æ°—", "æœ€é«˜æ°—æ¸©", "æœ€ä½æ°—æ¸©", "äºˆæ¸¬å£²ä¸Š"]
FIXED_PRODUCT_COLUMNS = [
    "01 PBAé‡‘ã®æˆ¿ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒãƒŠãƒŠãƒŸãƒ«ã‚¯",
    "02 BAå®Œç†ŸãƒãƒŠãƒŠãƒŸãƒ«ã‚¯",
    "03 STBAã¤ã¶ã¤ã¶ã„ã¡ã”ãƒãƒŠãƒŠãƒŸãƒ«ã‚¯",
    "04 MIXãƒˆãƒ­ãƒ”ã‚«ãƒ«ãƒãƒ³ã‚´ãƒ¼ãƒŸãƒƒã‚¯ã‚¹",
    "05 KBã‚±ãƒ¼ãƒ«ãƒãƒŠãƒŠ",
    "06 ACAIã‚¢ã‚µã‚¤ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¸ãƒ¼",
    "07 OPæœå®ŸãŸã£ã·ã‚Šã‚ªãƒ¬ãƒ³ã‚¸ãƒ‘ã‚¤ãƒ³",
    "08 KOPã¤ã¶ã¤ã¶ã‚­ã‚¦ã‚¤ã‚ªãƒ¬ãƒ³ã‚¸ãƒ‘ã‚¤ãƒ³",
    "09 BOCãƒ™ãƒªãƒ¼ãƒ™ãƒªãƒ¼ã‚ªãƒ¬ãƒ³ã‚¸ã‚³ã‚³ãƒŠãƒƒãƒ„",
    "10 MGã¤ã¶ã¤ã¶ãƒãƒ³ã‚´ãƒ¼ãƒŸãƒ«ã‚¯",
    "11 KIWIã”ã‚ã”ã‚ã‚­ã‚¦ã‚¤",
    "12 LMNã‚´ã‚¯ã‚´ã‚¯ãƒ¬ãƒ¢ãƒãƒ¼ãƒ‰ã‚½ãƒ¼ãƒ€",
    "13 LLSæ¾ã‚ŠãŸã¦ãƒ¬ãƒ¢ãƒ³ãƒ©ã‚¤ãƒ ã‚½ãƒ¼ãƒ€",
    "14 PGSæ¾ã‚ŠãŸã¦ãƒ”ãƒ³ã‚¯ã‚°ãƒ¬ãƒ¼ãƒ—ãƒ•ãƒ«ãƒ¼ãƒ„ã‚½ãƒ¼ãƒ€",
    "BLS ãƒ–ãƒ«ãƒ¼ãƒ¬ãƒ¢ãƒ³ã‚½ãƒ¼ãƒ€",
    "SS æ±äº¬ã‚µãƒ³ã‚»ãƒƒãƒˆã‚½ãƒ¼ãƒ€",
    "GY ã‚°ãƒªãƒ¼ã‚¯ãƒ¨ãƒ¼ã‚°ãƒ«ãƒˆ",
    "SU100 å›å³¶è¾²åœ’ã™ã„ã‹100%ç”Ÿçµã‚Šã‚¸ãƒ¥ãƒ¼ã‚¹",
    "MS ã¤ã¶ã¤ã¶ãƒ¡ãƒ­ãƒ³ã‚·ã‚§ã‚¤ã‚¯",
    "PSæ¡ƒã‚¹ãƒ ãƒ¼ã‚¸ãƒ¼",
    "SU100 ã‚ã¹è¾²åœ’ã™ã„ã‹100%ç”Ÿçµã‚Šã‚¸ãƒ¥ãƒ¼ã‚¹",
    "NS100 åˆ‡ã‚ŠãŸã¦æ¢¨100%ç”Ÿæ¾ã‚Šã‚¸ãƒ¥ãƒ¼ã‚¹",
    "KPS ã¾ã‚‹ã”ã¨å·¨å³°ã¨ãƒ‘ã‚¤ãƒ³ã‚¹ãƒ ãƒ¼ã‚¸ãƒ¼",
    "MK100 æ¥µæ—©ç”Ÿã¿ã‹ã‚“æœæ±100%ã‚¸ãƒ¥ãƒ¼ã‚¹",
    "IMO èœœã„ã‚‚ãƒŸãƒ«ã‚¯ã‚·ã‚§ã‚¤ã‚¯",
    "ACAIB ã‚¢ã‚µã‚¤ãƒ¼ãƒœã‚¦ãƒ«", # APKç½®ããŸã„æš«å®šã§ãŠã„ã¦ã‚‹
    "ã€BFé™å®šã€‘GKB é»’ã‚´ãƒããªã“ã®ãƒãƒŠãƒŠãƒŸãƒ«ã‚¯", 
]

# ========= UI =========
st.set_page_config(page_title="å£²ä¸Šãƒ»å•†å“æ•°äºˆæ¸¬ã‚¢ãƒ—ãƒª", layout="wide")
st.title("å£²ä¸Šãƒ»å•†å“æ•°äºˆæ¸¬ã‚¢ãƒ—ãƒª")

selected_dates = st.date_input("äºˆæ¸¬ã—ãŸã„æ—¥ä»˜ã‚’é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰", [], format="YYYY-MM-DD")

if isinstance(selected_dates, tuple):
    if len(selected_dates) == 2:
        start_date, end_date = selected_dates
        selected_dates = [start_date + datetime.timedelta(days=i) for i in range((end_date - start_date).days + 1)]
    else:
        selected_dates = list(selected_dates)
elif isinstance(selected_dates, datetime.date):
    selected_dates = [selected_dates]

# ---- ä¸–ç•Œã®ç¥æ—¥ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå¤©æ°—å…¥åŠ›ã®å‰ï¼‰----
if selected_dates:
    st.write("### ğŸŒ å›½æ¨ªæ–­ï¼šãã®æ—¥ãŒã©ã“ã®å›½ã®ç¥æ—¥ãƒ»é•·æœŸé€£ä¼‘ã«å½“ãŸã‚‹ã‹ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰")

    if pyholidays is None:
        st.info("ä¸–ç•Œã®ç¥æ—¥åˆ¤å®šã«ã¯ 'holidays' ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™ã€‚requirements.txt ã« 'holidays>=0.57' ã‚’è¿½åŠ å¾Œã€å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

    rows = []
    for d in selected_dates:
        hits = get_international_holidays(d)  # ç¥æ—¥åãƒ’ãƒƒãƒˆï¼ˆå›½åï¼šç¥æ—¥åï¼‰
        long_hits = []
        # å…¨å¯¾è±¡å›½ã§ã€Œé•·æœŸé€£ä¼‘ã€ãƒ’ãƒƒãƒˆã‚’æ‹¾ã†ï¼ˆç¥æ—¥åãŒç„¡ãã¦ã‚‚é€±æœ«åˆä½“ã§3é€£ä¼‘+ãªã‚‰ãƒ’ãƒƒãƒˆã•ã›ã‚‹ï¼‰
        for code, label in COUNTRIES.items():
            try:
                if is_long_holiday_in_country(d, code):
                    long_hits.append(f"{label}ï¼šé•·æœŸé€£ä¼‘")
            except Exception:
                continue

        status = " / ".join(hits + long_hits) if (hits or long_hits) else "è©²å½“ãªã—"
        rows.append({"æ—¥ä»˜": d.strftime("%Y-%m-%d"), "è©²å½“å›½ã®ç¥æ—¥ãƒ»é•·æœŸé€£ä¼‘": status})

    st.dataframe(pd.DataFrame(rows), use_container_width=True)

    # è©³ç´°ï¼ˆå›½åˆ¥ï¼‰ã‚’æŠ˜ã‚Šç•³ã¿ã§
    with st.expander("å›½åˆ¥ã®è©³ç´°ï¼ˆç¥æ—¥åï¼é•·æœŸé€£ä¼‘ãƒ’ãƒƒãƒˆï¼‰"):
        detail_rows = []
        for d in selected_dates:
            # 1å›ã ã‘å–å¾—ã—ã¦ã‹ã‚‰å›½åˆ¥ã«æ•´å½¢ï¼ˆç„¡é§„ãªå†è¨ˆç®—ã‚’æ¸›ã‚‰ã™ï¼‰
            names_all = get_international_holidays(d)  # ['ä¸­å›½ï¼šæ˜¥èŠ‚', 'ã‚¢ãƒ¡ãƒªã‚«ï¼šIndependence Day', ...]
            by_country = {}
            for s in names_all:
                if "ï¼š" in s:
                    label, name = s.split("ï¼š", 1)
                    by_country[label] = (by_country.get(label, []) + [name])
            for code, label in COUNTRIES.items():
                long_f = is_long_holiday_in_country(d, code)
                if (label in by_country) or long_f:
                    detail_rows.append({
                        "æ—¥ä»˜": d.strftime("%Y-%m-%d"),
                        "å›½": label,
                        "ç¥æ—¥å": " / ".join(by_country.get(label, [])),
                        "é•·æœŸé€£ä¼‘": "â—¯" if long_f else ""
                    })
        if detail_rows:
            st.dataframe(pd.DataFrame(detail_rows), use_container_width=True)
        else:
            st.info("è©²å½“ãªã—")

# ---- ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆ/ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£/ãŠå°å ´ ã‚¤ãƒ™ãƒ³ãƒˆå€™è£œï¼ˆæ—¥æœ¬èªã®ã¿ï¼‰ ----
if selected_dates:
    st.write("### ğŸª ãƒ“ãƒƒã‚°ã‚µã‚¤ãƒˆï¼ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£æ±äº¬ãƒ—ãƒ©ã‚¶ï¼ãŠå°å ´ï¼šã‚¤ãƒ™ãƒ³ãƒˆé–‹å‚¬ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæ—¥æœ¬èªï¼‰")

    event_rows = []
    for d in selected_dates:
        found = _scan_event_pages_jp(d)
        found = _filter_big_events(found)
        if found:
            for ev in found:
                event_rows.append({
                    "æ—¥ä»˜": d.strftime("%Y-%m-%d"),
                    "ä¼šå ´": ev["ä¼šå ´"],
                    "é–‹å§‹æ—¥": ev["é–‹å§‹æ—¥"],
                    "çµ‚äº†æ—¥": ev["çµ‚äº†æ—¥"],
                    "ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæŠœç²‹ï¼‰": ev["ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæŠœç²‹ï¼‰"],
                    "ãƒªãƒ³ã‚¯": ev["ãƒªãƒ³ã‚¯"],
                })
        else:
            event_rows.append({
                "æ—¥ä»˜": d.strftime("%Y-%m-%d"),
                "ä¼šå ´": "-",
                "é–‹å§‹æ—¥": "",
                "çµ‚äº†æ—¥": "",
                "ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆæŠœç²‹ï¼‰": "è©²å½“ãªã—",
                "ãƒªãƒ³ã‚¯": "",
            })

    df_events = pd.DataFrame(event_rows)
    try:
        st.dataframe(
            df_events,
            use_container_width=True,
            column_config={"ãƒªãƒ³ã‚¯": st.column_config.LinkColumn("ãƒªãƒ³ã‚¯")}
        )
    except Exception:
        # å¤ã„Streamlitãªã©ã§ LinkColumn ãŒç„¡ã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        st.dataframe(df_events, use_container_width=True)

    st.caption("â€» å…¬å¼ã‚µã‚¤ãƒˆã®ä¸€è¦§/ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰æ—¥ä»˜è¡¨è¨˜ã‚’æŠ½å‡ºã—ã¦ã„ã¾ã™ã€‚è¡¨è¨˜ã‚†ã‚Œã«ã‚ˆã‚Šå–ã‚Šã“ã¼ã™å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

# ---- ä»¥é™ã¯æ—¢å­˜ã©ãŠã‚Šï¼ˆå¤©æ°—ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼â†’å…¥åŠ›â†’äºˆæ¸¬ï¼‰----
selected_season = []
if selected_dates:
    st.write("### ğŸŒ¤ï¸ é¸æŠæ—¥ä»˜ã®å¤©æ°—ã¨æ°—æ¸©ï¼ˆå‰å¹´ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚€ï¼‰")
    weather_rows = []
    today = datetime.date.today()

    for date in selected_dates:
        delta = (date - today).days
        if delta <= 5:
            weather, temp_max, temp_min = fetch_weather_forecast(date)
        else:
            weather, temp_max, temp_min = "", "", ""

        prev_date = date.replace(year=date.year - 1)
        prev = df_prev_weather[df_prev_weather["date"] == pd.Timestamp(prev_date)]
        if not prev.empty:
            prev_weather = prev.iloc[0]["weather"]
            prev_max = prev.iloc[0]["temp_max"]
            prev_min = prev.iloc[0]["temp_min"]
        else:
            prev_weather, prev_max, prev_min = "", "", ""

        weather_rows.append({
            "æ—¥ä»˜": date.strftime("%Y-%m-%d"),
            "å¤©æ°—": weather,
            "æœ€é«˜æ°—æ¸©": temp_max,
            "æœ€ä½æ°—æ¸©": temp_min,
            "æ˜¨å¹´ã®å¤©æ°—": prev_weather,
            "æ˜¨å¹´ã®æœ€é«˜æ°—æ¸©": prev_max,
            "æ˜¨å¹´ã®æœ€ä½æ°—æ¸©": prev_min,
        })

    st.dataframe(pd.DataFrame(weather_rows), use_container_width=True)

    st.write("### å„æ—¥ä»˜ã®æƒ…å ±å…¥åŠ›")
    date_inputs = []
    for date in selected_dates:
        with st.expander(f"{date.strftime('%Y-%m-%d')} ã®è¨­å®š"):
            delta = (date - today).days
            if delta <= 7:
                weather, temp_max, temp_min = fetch_weather_forecast(date)
                if weather is None:
                    st.warning("å¤©æ°—å–å¾—å¤±æ•—ã€‚æ‰‹å‹•ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    temp_max = st.number_input("æœ€é«˜æ°—æ¸©", key=f"max_manual_{date}", step=1, value=20)
                    temp_min = st.number_input("æœ€ä½æ°—æ¸©", key=f"min_manual_{date}", step=1, value=20)
                    weather = st.selectbox("å¤©æ°—", ["æ™´ã‚Œ", "æ›‡ã‚Š", "é›¨"], key=f"weather_manual_{date}")
                else:
                    # è‹±èªå¤©æ°—â†’å’Œåå€™è£œ
                    candidates = ["æ™´ã‚Œ", "æ›‡ã‚Š", "é›¨"]
                    default = 0
                    if weather in ["Clear", "Clouds", "Rain"]:
                        default = ["Clear", "Clouds", "Rain"].index(weather)
                    weather = st.selectbox("å¤©æ°—", candidates, index=default, key=f"weather_{date}")
                    temp_max = st.number_input("æœ€é«˜æ°—æ¸©", key=f"max_{date}", value=int(temp_max) if temp_max else 20)
                    temp_min = st.number_input("æœ€ä½æ°—æ¸©", key=f"min_{date}", value=int(temp_min) if temp_min else 20)
            else:
                weather = st.selectbox("å¤©æ°—", ["æ™´ã‚Œ", "æ›‡ã‚Š", "é›¨"], key=f"weather_{date}")
                temp_max = st.number_input("æœ€é«˜æ°—æ¸©", key=f"max_{date}", value=20)
                temp_min = st.number_input("æœ€ä½æ°—æ¸©", key=f"min_{date}", value=20)
            event = st.checkbox("ã‚¤ãƒ™ãƒ³ãƒˆæœ‰ç„¡", key=f"event_{date}")
            date_inputs.append({"date": date, "weather": weather, "temp_max": temp_max, "temp_min": temp_min, "event": int(event)})

    st.write("### ã‚·ãƒ¼ã‚ºãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠ")
    with st.expander("ã‚·ãƒ¼ã‚ºãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’é¸ã¶"):
        selected_season = st.multiselect("å‡ºåŠ›ã™ã‚‹ã‚·ãƒ¼ã‚ºãƒ³ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’é¸ã‚“ã§ãã ã•ã„", seasonal_items_all)

# ========= å®Ÿè¡Œãƒœã‚¿ãƒ³ =========
if st.button("äºˆæ¸¬ã‚’å®Ÿè¡Œ"):
    if not selected_dates:
        st.warning("æ—¥ä»˜ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    rows_for_table = []
    all_products_used = set()  # äºˆæ¸¬ã§è§¦ã‚ŒãŸå…¨å•†å“ï¼ˆåˆ—è¿½åŠ ã®ãŸã‚ï¼‰

    for entry in date_inputs:
        date_str = entry['date'].strftime('%Y-%m-%d')
        feat = make_features(entry)

        # å£²ä¸Šäºˆæ¸¬ï¼ˆâ€» multiplier ã¯ã”æç¤ºã©ãŠã‚Š 1.0 ã®ã¾ã¾ï¼‰
        raw_sales = sales_model.predict(feat.drop(columns=["å£²ä¸Š", "ç¹å¿™æœŸãƒ•ãƒ©ã‚°"]))[0]
        multiplier = 1.0 if feat.at[0, "ç¹å¿™æœŸãƒ•ãƒ©ã‚°"] == 1 else 1.0
        pred_sales = int(raw_sales * multiplier)

        # å•†å“æ•°é‡äºˆæ¸¬
        feat["å£²ä¸Š"] = pred_sales
        qty_dict = {}
        for item in (constant_items + selected_season):
            if item in product_models:
                qty = int(product_models[item].predict(feat.drop(columns=["ç¹å¿™æœŸãƒ•ãƒ©ã‚°"]))[0])
                qty_dict[item] = qty
            # ãƒ¢ãƒ‡ãƒ«ãªã—ã¯æœªå‡ºåŠ›ã«ã—ã¦OKï¼ˆåˆ—ã¯å¾Œã§ä½œã‚‹ãŒå€¤ã¯ç©ºï¼‰
            all_products_used.add(item)

        # è¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆåŸºæœ¬é …ç›®ï¼‰
        weekday_jp = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"][entry["date"].weekday()]
        row = {
            "æ—¥ä»˜": date_str,
            "æ›œæ—¥": weekday_jp,
            "å¤©æ°—": entry["weather"],
            "æœ€é«˜æ°—æ¸©": int(entry["temp_max"]) if entry["temp_max"] not in ["", None] else "",
            "æœ€ä½æ°—æ¸©": int(entry["temp_min"]) if entry["temp_min"] not in ["", None] else "",
            "äºˆæ¸¬å£²ä¸Š": pred_sales,
        }

        # å…ˆã«å›ºå®šåˆ—ï¼ˆæŒ‡å®šé †ï¼‰ã‚’åŸ‹ã‚ã‚‹
        for col in FIXED_PRODUCT_COLUMNS:
            row[col] = qty_dict.get(col, "")

        # å¾Œã‚ã«â€œãã®ä»–å•†å“â€ã‚’ä¸¦ã¹ã‚‹ï¼ˆPSæ¡ƒã‚¹ãƒ ãƒ¼ã‚¸ãƒ¼ã®å¾Œï¼‰
        others = sorted(list(all_products_used - set(FIXED_PRODUCT_COLUMNS)))
        for prod in others:
            row[prod] = qty_dict.get(prod, "")

        rows_for_table.append(row)

    # åˆ—æ§‹æˆï¼šåŸºæœ¬ â†’ å›ºå®šå•†å“ â†’ ãã®ä»–å•†å“ï¼ˆPSæ¡ƒã‚¹ãƒ ãƒ¼ã‚¸ãƒ¼ã®å¾Œï¼‰
    all_other_products = sorted(list((all_products_used - set(FIXED_PRODUCT_COLUMNS))))
    final_columns = BASE_COLUMNS + FIXED_PRODUCT_COLUMNS + all_other_products

    df_out = pd.DataFrame(rows_for_table)
    # è¶³ã‚Šãªã„åˆ—ã¯ä½œã£ã¦é †åºã‚’æƒãˆã‚‹
    for c in final_columns:
        if c not in df_out.columns:
            df_out[c] = ""
    df_out = df_out[final_columns]

    st.write("## ğŸ“‹ ã‚³ãƒ”ãƒšç”¨ã®çµæœè¡¨ï¼ˆã“ã®ã¾ã¾Excelã¸è²¼ã‚Šä»˜ã‘å¯ï¼‰")
    st.dataframe(df_out, use_container_width=True)

    st.write("#### ã‚¿ãƒ–åŒºåˆ‡ã‚Šãƒ†ã‚­ã‚¹ãƒˆï¼ˆCtrl/Cmd + A â†’ ã‚³ãƒ”ãƒ¼ â†’ Excelã«è²¼ã‚Šä»˜ã‘ï¼‰")
    tsv = df_out.to_csv(sep="\t", index=False)
    st.text_area("TSV", tsv, height=200)

    st.download_button(
        "CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=df_out.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"pred_{datetime.date.today().isoformat()}.csv",
        mime="text/csv"
    )
